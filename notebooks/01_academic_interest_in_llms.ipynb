{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Interest in LLMs\n",
    "\n",
    "**Purpose**: <br>\n",
    "<br>\n",
    "To use [arxiv](https://arxiv.org/) metadata to track the interest in LLMs by way word-count references.\n",
    "\n",
    "**Instructions**: <br>\n",
    "1. To follow along yourself, you will need access to a [Kaggle](https://www.kaggle.com/) account. after signing up, download your API key from account -> settings -> \"Create New Token\". Ensure the downloaded `kaggle.json` file, which contains your username and API key, is placed in a `.kaggle` folder in your root directory. e.g. `mv ~/Downloads/ ~/.kaggle/kaggle.json`.\n",
    "\n",
    "2. Ensure your kaggle file has the correct permissions: `chmod 600 ~/.kaggle/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading arxiv.zip to ../data\n",
      " 99%|█████████████████████████████████████▋| 1.15G/1.16G [00:19<00:00, 90.7MB/s]\n",
      "100%|██████████████████████████████████████| 1.16G/1.16G [00:20<00:00, 61.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -p ../data/ cornell-university/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../data/arxiv.zip\n",
      "  inflating: ../data/arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "! unzip ../data/arxiv.zip -d ../data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the just into a list of dictionaries instead of a string representation of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "with open('../data/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    data = [loads(line) for line in f]\n",
    "    f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if anything from title or abstract, contains information about LLM or closely-related subjects such as transformers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7391"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def check_for_references(meta_data: Dict[str, str]):\n",
    "    \"\"\"If none of the key phrases are in the text, return False\"\"\"\n",
    "    key_phrases = [\n",
    "                    \"large language model\",\n",
    "                    \"large language models\",\n",
    "                    \"LLM\",\n",
    "                    \"LLMs\",\n",
    "                    \"Attention Is All You Need\",\n",
    "                    \"generative ai\",\n",
    "                    \"GPT-3\",\n",
    "                    \"GPT-4\",\n",
    "                    \"OpenAI\",\n",
    "                    \"Transformer architecture\",\n",
    "                    \"transformers\",\n",
    "                    \"self-attention\",\n",
    "                ]\n",
    "    bert_like_models = set([\n",
    "                        \"BERT\",\n",
    "                        \"RoBERTa\",\n",
    "                        \"DistilBERT\",\n",
    "                        \"ALBERT\",\n",
    "                        \"SpanBERT\",\n",
    "                        \"BioBERT\",\n",
    "                        \"SciBERT\",\n",
    "                        \"CamemBERT\",\n",
    "                        \"TurkuBERT\",\n",
    "                        \"MobileBERT\",\n",
    "                        \"TinyBERT\",\n",
    "                        \"ELECTRA\",\n",
    "                        \"DeBERTa\"\n",
    "                    ])\n",
    "\n",
    "    key_phrases = set([phrase.lower() for phrase in key_phrases])\n",
    "\n",
    "    title: str = meta_data['title'].replace('\\n', ' ').lower().split()\n",
    "    abstract: str = meta_data['abstract'].replace('\\n', ' ').lower().split()\n",
    "\n",
    "    for phrase in key_phrases:\n",
    "        if (phrase in title) or (phrase in abstract):\n",
    "            return True\n",
    "        \n",
    "    for phrase in  bert_like_models:\n",
    "        if (phrase in title) or (phrase in abstract):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "sum([1 for doc in data if check_for_references(doc)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we know there are plenty of scholarly interest in the above. Frankly, it'd be surprising if there weren't... \n",
    "\n",
    "Next, let's try to find a timeline of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id           submitter  \\\n",
       "0  0704.0001      Pavel Nadolsky   \n",
       "1  0704.0002        Louis Theran   \n",
       "2  0704.0003         Hongjun Pan   \n",
       "3  0704.0004        David Callan   \n",
       "4  0704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  2008-01-13   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2007-05-23   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2013-10-15   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  \n",
       "2                                 [[Pan, Hongjun, ]]  \n",
       "3                                [[Callan, David, ]]  \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "# First, we load the data into a dataframe. Since it's a list of dictionaries, we can use the from_records function.\n",
    "\n",
    "metadata = DataFrame.from_records(data)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I'd like to determine when the _first_ version of these articles were published, so we'll parse out that information\n",
    "\n",
    "v1_dates = [version[0]['created'] for version in metadata['versions'] if version[0]['version'] == 'v1']\n",
    "metadata['v1_dates'] = v1_dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: What's nice about the above is, despite putting a condition of version == 'v1', since the dataframe was able to create a new column, we know implcitly there were no articles missing a v1 date. Python would have thrown an error saying v1_dates was too short, otherwise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's add a boolean to each row that contains an LLM-like reference. We will use these later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['is_LLM'] = [1 if check_for_references(doc) else 0 for doc in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>On over-reflection and generation of Gravito-A...</td>\n",
       "      <td>The dynamics of linear perturbations is stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210</th>\n",
       "      <td>IIB backgrounds with five-form flux</td>\n",
       "      <td>We investigate all N=2 supersymmetric IIB su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Anatomy of bubbling solutions</td>\n",
       "      <td>We present a comprehensive analysis of holog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701</th>\n",
       "      <td>Self-Stabilizing Wavelets and r-Hops Coordination</td>\n",
       "      <td>We introduce a simple tool called the wavele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17627</th>\n",
       "      <td>Long-time stable HTSC DC-SQUID gradiometers wi...</td>\n",
       "      <td>In applications for high-Tc superconducting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title                                           abstract\n",
       "3918   On over-reflection and generation of Gravito-A...    The dynamics of linear perturbations is stud...\n",
       "6210                 IIB backgrounds with five-form flux    We investigate all N=2 supersymmetric IIB su...\n",
       "8902                       Anatomy of bubbling solutions    We present a comprehensive analysis of holog...\n",
       "12701  Self-Stabilizing Wavelets and r-Hops Coordination    We introduce a simple tool called the wavele...\n",
       "17627  Long-time stable HTSC DC-SQUID gradiometers wi...    In applications for high-Tc superconducting ..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata['is_LLM'] == 1][['title', 'abstract']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  We investigate all N=2 supersymmetric IIB supergravity backgrounds with\\nnon-vanishing five-form flux. The Killing spinors have stability subgroups\\n$Spin(7)\\\\ltimes\\\\bR^8$, $SU(4)\\\\ltimes\\\\bR^8$ and $G_2$. In the\\n$SU(4)\\\\ltimes\\\\bR^8$ case, two different types of geometry arise depending on\\nwhether the Killing spinors are generic or pure. In both cases, the backgrounds\\nadmit a null Killing vector field which leaves invariant the $SU(4)\\\\ltimes\\n\\\\bR^8$ structure, and an almost complex structure in the directions transverse\\nto the lightcone. In the generic case, the twist of the vector field is trivial\\nbut the almost complex structure is non-integrable, while in the pure case the\\ntwist is non-trivial but the almost complex structure is integrable and\\nassociated with a relatively balanced Hermitian structure. The $G_2$\\nbackgrounds admit a time-like Killing vector field and two spacelike closed\\none-forms, and the seven directions transverse to these admit a co-symplectic\\n$G_2$ structure. The $Spin(7)\\\\ltimes\\\\bR^8$ backgrounds are pp-waves propagating\\nin an eight-dimensional manifold with holonomy $Spin(7)$. In addition we show\\nthat all the supersymmetric solutions of simple five-dimensional supergravity\\nwith a time-like Killing vector field, which include the $AdS_5$ black holes,\\nlift to $SU(4)\\\\ltimes\\\\bR^8$ pure Killing spinor IIB backgrounds. We also show\\nthat the LLM solution is associated with a co-symplectic co-homogeneity one\\n$G_2$ manifold which has principal orbit $S^3\\\\times S^3$.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata['is_LLM'] == 1][['title', 'abstract']].iat[1, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see that the results are full of false positives. Let's use something a bit more adanced, and relative to our work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\n",
    "\n",
    "# Let's say we have a list of abstracts\n",
    "abstracts = [\n",
    "    'This paper presents a new approach to generative AI using large language models.',\n",
    "    'We propose a novel architecture for transformer models in NLP tasks.',\n",
    "    # Add more abstracts here...\n",
    "]\n",
    "\n",
    "# Create a system persona that explains the task\n",
    "system_persona = \"Your task is to classify whether an abstract is about Large Language Models (LLMs) and Generative AI, or not.\"\n",
    "\n",
    "\n",
    "for abstract in abstracts:\n",
    "    # Create a conversation with the system persona and the abstract\n",
    "    conversation = Conversation(system_persona + \"\\n\" + abstract)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    model_input = tokenizer(conversation, return_tensors='pt')\n",
    "    model_output = model.generate(**model_input)\n",
    "    response = tokenizer.decode(model_output[:, model_input['input_ids'].shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the response\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27fe8cb375d4e7db0d20be9420e337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type RefinedWebModel to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d02acc9df474cdf93b032a8fa5fe656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b61cd821e154a389185b0ab25f3b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b89b27982f84b28a0284d80af25f730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852d245b3d1c4966bc1ae1267a16640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f3803c29b047f4979e0bf52fa84ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tiiuae/falcon-7b-instruct were not used when initializing GPT2LMHeadModel: ['transformer.h.30.input_layernorm.bias', 'transformer.h.9.input_layernorm.bias', 'transformer.h.29.self_attention.dense.weight', 'transformer.h.0.self_attention.dense.weight', 'transformer.h.15.self_attention.dense.weight', 'transformer.h.23.input_layernorm.bias', 'transformer.h.23.mlp.dense_h_to_4h.weight', 'transformer.h.5.self_attention.query_key_value.weight', 'transformer.h.28.mlp.dense_4h_to_h.weight', 'transformer.h.9.self_attention.dense.weight', 'transformer.h.23.self_attention.dense.weight', 'transformer.h.6.self_attention.query_key_value.weight', 'transformer.h.10.input_layernorm.weight', 'transformer.h.28.mlp.dense_h_to_4h.weight', 'transformer.h.29.input_layernorm.bias', 'transformer.h.11.input_layernorm.bias', 'transformer.h.14.input_layernorm.weight', 'transformer.h.21.input_layernorm.bias', 'transformer.h.4.mlp.dense_4h_to_h.weight', 'transformer.h.3.mlp.dense_4h_to_h.weight', 'transformer.h.18.mlp.dense_h_to_4h.weight', 'transformer.h.27.mlp.dense_h_to_4h.weight', 'transformer.h.10.mlp.dense_h_to_4h.weight', 'transformer.h.24.self_attention.query_key_value.weight', 'transformer.h.27.mlp.dense_4h_to_h.weight', 'transformer.h.12.self_attention.query_key_value.weight', 'transformer.h.18.self_attention.query_key_value.weight', 'transformer.h.23.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_4h_to_h.weight', 'transformer.h.24.input_layernorm.bias', 'transformer.h.6.self_attention.dense.weight', 'transformer.h.20.input_layernorm.bias', 'transformer.h.20.mlp.dense_h_to_4h.weight', 'transformer.h.21.self_attention.query_key_value.weight', 'transformer.h.17.mlp.dense_4h_to_h.weight', 'transformer.h.4.input_layernorm.weight', 'transformer.h.28.self_attention.dense.weight', 'transformer.h.31.mlp.dense_h_to_4h.weight', 'transformer.h.22.input_layernorm.weight', 'transformer.h.13.input_layernorm.bias', 'transformer.h.22.self_attention.query_key_value.weight', 'transformer.h.2.input_layernorm.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight', 'transformer.h.23.self_attention.query_key_value.weight', 'transformer.h.26.mlp.dense_4h_to_h.weight', 'transformer.h.14.self_attention.dense.weight', 'transformer.h.28.self_attention.query_key_value.weight', 'transformer.h.25.input_layernorm.weight', 'transformer.h.4.self_attention.query_key_value.weight', 'transformer.h.0.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_h_to_4h.weight', 'transformer.h.18.mlp.dense_4h_to_h.weight', 'transformer.h.2.mlp.dense_4h_to_h.weight', 'transformer.h.24.input_layernorm.weight', 'transformer.h.10.input_layernorm.bias', 'transformer.h.3.input_layernorm.bias', 'transformer.h.8.self_attention.query_key_value.weight', 'transformer.h.9.input_layernorm.weight', 'transformer.h.30.self_attention.query_key_value.weight', 'transformer.h.6.mlp.dense_4h_to_h.weight', 'transformer.h.7.self_attention.dense.weight', 'transformer.h.21.mlp.dense_h_to_4h.weight', 'transformer.h.12.mlp.dense_h_to_4h.weight', 'transformer.h.29.mlp.dense_4h_to_h.weight', 'transformer.h.5.mlp.dense_h_to_4h.weight', 'transformer.h.30.input_layernorm.weight', 'transformer.h.29.input_layernorm.weight', 'transformer.h.7.input_layernorm.weight', 'transformer.h.1.mlp.dense_h_to_4h.weight', 'transformer.h.29.mlp.dense_h_to_4h.weight', 'transformer.h.19.self_attention.dense.weight', 'transformer.h.31.self_attention.query_key_value.weight', 'transformer.h.11.mlp.dense_h_to_4h.weight', 'transformer.h.15.mlp.dense_4h_to_h.weight', 'transformer.h.15.mlp.dense_h_to_4h.weight', 'transformer.h.16.input_layernorm.bias', 'transformer.h.2.self_attention.query_key_value.weight', 'transformer.h.1.self_attention.dense.weight', 'transformer.h.20.mlp.dense_4h_to_h.weight', 'transformer.h.15.self_attention.query_key_value.weight', 'transformer.h.5.mlp.dense_4h_to_h.weight', 'transformer.h.15.input_layernorm.weight', 'transformer.h.16.mlp.dense_4h_to_h.weight', 'transformer.h.3.self_attention.dense.weight', 'transformer.h.18.self_attention.dense.weight', 'transformer.h.7.input_layernorm.bias', 'transformer.h.12.input_layernorm.bias', 'transformer.h.26.self_attention.dense.weight', 'transformer.h.1.input_layernorm.weight', 'transformer.h.13.self_attention.dense.weight', 'transformer.h.4.self_attention.dense.weight', 'transformer.h.13.mlp.dense_h_to_4h.weight', 'transformer.h.5.self_attention.dense.weight', 'transformer.h.18.input_layernorm.bias', 'transformer.h.26.mlp.dense_h_to_4h.weight', 'transformer.h.19.mlp.dense_h_to_4h.weight', 'transformer.h.22.self_attention.dense.weight', 'transformer.h.17.input_layernorm.weight', 'transformer.h.17.self_attention.dense.weight', 'transformer.h.5.input_layernorm.weight', 'transformer.h.14.input_layernorm.bias', 'transformer.h.18.input_layernorm.weight', 'transformer.h.13.mlp.dense_4h_to_h.weight', 'transformer.h.14.self_attention.query_key_value.weight', 'transformer.h.27.self_attention.dense.weight', 'transformer.h.12.input_layernorm.weight', 'transformer.h.11.self_attention.dense.weight', 'transformer.h.1.mlp.dense_4h_to_h.weight', 'transformer.h.7.mlp.dense_4h_to_h.weight', 'transformer.h.8.mlp.dense_h_to_4h.weight', 'transformer.h.20.input_layernorm.weight', 'transformer.h.24.self_attention.dense.weight', 'transformer.h.26.self_attention.query_key_value.weight', 'transformer.h.30.self_attention.dense.weight', 'transformer.h.12.self_attention.dense.weight', 'transformer.h.22.input_layernorm.bias', 'transformer.h.19.self_attention.query_key_value.weight', 'transformer.h.16.self_attention.query_key_value.weight', 'transformer.h.3.mlp.dense_h_to_4h.weight', 'transformer.h.13.self_attention.query_key_value.weight', 'transformer.h.25.self_attention.dense.weight', 'transformer.h.2.mlp.dense_h_to_4h.weight', 'transformer.h.20.self_attention.query_key_value.weight', 'transformer.h.31.input_layernorm.weight', 'transformer.h.9.mlp.dense_4h_to_h.weight', 'transformer.h.1.self_attention.query_key_value.weight', 'transformer.h.12.mlp.dense_4h_to_h.weight', 'transformer.h.21.mlp.dense_4h_to_h.weight', 'transformer.h.30.mlp.dense_4h_to_h.weight', 'transformer.h.21.self_attention.dense.weight', 'transformer.h.19.input_layernorm.weight', 'transformer.h.2.self_attention.dense.weight', 'transformer.h.3.self_attention.query_key_value.weight', 'transformer.h.1.input_layernorm.bias', 'transformer.h.27.self_attention.query_key_value.weight', 'transformer.h.25.mlp.dense_h_to_4h.weight', 'transformer.h.10.self_attention.dense.weight', 'transformer.h.13.input_layernorm.weight', 'transformer.h.24.mlp.dense_h_to_4h.weight', 'transformer.h.28.input_layernorm.weight', 'transformer.h.31.input_layernorm.bias', 'transformer.h.17.mlp.dense_h_to_4h.weight', 'transformer.h.22.mlp.dense_4h_to_h.weight', 'transformer.h.30.mlp.dense_h_to_4h.weight', 'transformer.h.15.input_layernorm.bias', 'transformer.h.25.input_layernorm.bias', 'transformer.h.28.input_layernorm.bias', 'transformer.h.20.self_attention.dense.weight', 'transformer.h.23.input_layernorm.weight', 'transformer.h.25.self_attention.query_key_value.weight', 'transformer.h.16.input_layernorm.weight', 'transformer.h.11.self_attention.query_key_value.weight', 'transformer.h.26.input_layernorm.weight', 'transformer.h.22.mlp.dense_h_to_4h.weight', 'transformer.h.10.mlp.dense_4h_to_h.weight', 'transformer.h.31.self_attention.dense.weight', 'transformer.h.27.input_layernorm.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight', 'transformer.h.11.mlp.dense_4h_to_h.weight', 'transformer.h.10.self_attention.query_key_value.weight', 'transformer.h.0.self_attention.query_key_value.weight', 'transformer.h.11.input_layernorm.weight', 'transformer.h.6.input_layernorm.bias', 'transformer.h.9.self_attention.query_key_value.weight', 'transformer.h.16.mlp.dense_h_to_4h.weight', 'transformer.h.0.input_layernorm.weight', 'transformer.h.19.input_layernorm.bias', 'transformer.h.24.mlp.dense_4h_to_h.weight', 'transformer.h.16.self_attention.dense.weight', 'transformer.h.31.mlp.dense_4h_to_h.weight', 'transformer.h.3.input_layernorm.weight', 'transformer.h.7.self_attention.query_key_value.weight', 'transformer.h.21.input_layernorm.weight', 'transformer.word_embeddings.weight', 'transformer.h.8.mlp.dense_4h_to_h.weight', 'transformer.h.0.input_layernorm.bias', 'transformer.h.29.self_attention.query_key_value.weight', 'transformer.h.19.mlp.dense_4h_to_h.weight', 'transformer.h.8.self_attention.dense.weight', 'transformer.h.8.input_layernorm.weight', 'transformer.h.27.input_layernorm.weight', 'transformer.h.5.input_layernorm.bias', 'transformer.h.4.input_layernorm.bias', 'transformer.h.2.input_layernorm.weight', 'transformer.h.0.mlp.dense_h_to_4h.weight', 'transformer.h.9.mlp.dense_h_to_4h.weight', 'transformer.h.25.mlp.dense_4h_to_h.weight', 'transformer.h.26.input_layernorm.bias', 'transformer.h.17.input_layernorm.bias', 'transformer.h.6.input_layernorm.weight', 'transformer.h.4.mlp.dense_h_to_4h.weight', 'transformer.h.8.input_layernorm.bias', 'transformer.h.17.self_attention.query_key_value.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at tiiuae/falcon-7b-instruct and are newly initialized: ['transformer.h.10.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.14.ln_2.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.12.ln_2.bias', 'transformer.h.28.attn.c_attn.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.30.attn.c_proj.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.17.ln_1.bias', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.25.ln_1.weight', 'transformer.h.31.attn.c_proj.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.14.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.19.ln_2.bias', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.23.ln_1.weight', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.13.ln_1.bias', 'transformer.h.24.ln_2.bias', 'transformer.h.16.ln_2.weight', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.29.attn.c_attn.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.24.attn.c_attn.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.27.ln_1.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.15.ln_1.weight', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.26.attn.c_attn.weight', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.29.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.13.ln_1.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.11.ln_2.bias', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.13.ln_2.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.12.ln_1.weight', 'transformer.h.30.ln_1.bias', 'transformer.wpe.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.25.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.18.ln_2.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.9.ln_1.bias', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.20.ln_2.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.17.ln_2.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.25.ln_2.bias', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.31.attn.c_attn.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.16.ln_1.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.18.ln_1.weight', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.19.ln_2.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.30.ln_2.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.26.attn.c_proj.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.5.ln_2.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.30.ln_2.bias', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.23.ln_2.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.wte.weight', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.24.attn.c_proj.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.27.attn.c_attn.weight', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.25.attn.c_attn.weight', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.31.ln_1.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.28.ln_2.bias', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.25.ln_2.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.30.attn.c_attn.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.12.ln_2.weight', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.28.attn.c_proj.weight', 'transformer.h.26.ln_1.weight', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.29.ln_1.weight', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.4.ln_2.bias', 'transformer.h.28.ln_1.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.0.ln_1.bias', 'transformer.h.10.ln_1.bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.27.attn.c_proj.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.19.ln_1.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.27.ln_1.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.3.ln_1.bias', 'transformer.h.19.ln_1.bias', 'transformer.h.26.ln_2.bias', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.16.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.5.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.31.ln_1.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.31.ln_2.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.1.ln_1.weight', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.28.ln_2.weight', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.14.ln_2.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.17.ln_2.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.6.ln_1.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.29.mlp.c_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20155b4b7ab2415c89f8b06e2ea40f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 316.00 MiB (GPU 0; 23.67 GiB total capacity; 22.11 GiB already allocated; 238.50 MiB free; 22.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Load the model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mtiiuae/falcon-7b-instruct\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mtiiuae/falcon-7b-instruct\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Generate text\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1878\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1874\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1875\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1876\u001b[0m     )\n\u001b[1;32m   1877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Documents/Code/using_llms_in_business_oreilly/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 0; 23.67 GiB total capacity; 22.11 GiB already allocated; 238.50 MiB free; 22.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Check if a GPU is available and if not, we use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('tiiuae/falcon-7b-instruct').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('tiiuae/falcon-7b-instruct')\n",
    "\n",
    "# Generate text\n",
    "def generate_text(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(input_ids, max_length=1024, do_sample=True)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "device\n",
    "# input_text = \"Hello, how are you?\"\n",
    "# output_text = generate_text(input_text)\n",
    "# print(output_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's start grouping by monthly counts using my favorite pandas function, pd.Grouper. For more, you can check my article [here](https://benjaminlabaschin.com/pandas-functions-advanced-groupbys-with-grouper-assign-and-query/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2007-04-02 19:18:42\n",
       "1         2007-03-31 02:26:18\n",
       "2         2007-04-01 20:46:54\n",
       "3         2007-03-31 03:16:14\n",
       "4         2007-04-02 18:09:58\n",
       "                  ...        \n",
       "2263487   1996-08-26 15:08:35\n",
       "2263488   1996-08-31 17:34:38\n",
       "2263489   1996-09-03 14:08:26\n",
       "2263490   1996-09-18 07:57:29\n",
       "2263491   1996-09-25 14:17:09\n",
       "Name: v1_dates, Length: 2263492, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Grouper, to_datetime\n",
    "\n",
    "# first let's convert to a datetime object\n",
    "metadata['v1_dates'] = to_datetime(metadata['v1_dates'])\n",
    "\n",
    "metadata.groupby(Grouper(key='v1_dates', freq='1m'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7033eb4811a2f216895b3353bdf40b8d75810e55c69fdabadbe127de1c8ea4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
